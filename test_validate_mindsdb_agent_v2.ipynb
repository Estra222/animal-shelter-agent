{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b67cc19d",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import duckdb\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Configure paths\n",
    "PROJECT_DIR = r'c:\\Users\\mvzie\\Documents\\AI Agent Experiment'\n",
    "DB_PATH = os.path.join(PROJECT_DIR, 'animal_shelter.duckdb')\n",
    "GROUND_TRUTH_PATH = os.path.join(PROJECT_DIR, 'agent_ground_truth_test_cases.json')\n",
    "\n",
    "print(f\"Project Directory: {PROJECT_DIR}\")\n",
    "print(f\"Database Path: {DB_PATH}\")\n",
    "print(f\"Ground Truth Path: {GROUND_TRUTH_PATH}\")\n",
    "print(f\"\\nFiles exist:\")\n",
    "print(f\"  Database: {os.path.exists(DB_PATH)}\")\n",
    "print(f\"  Ground Truth: {os.path.exists(GROUND_TRUTH_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c4641",
   "metadata": {},
   "source": [
    "## Section 2: Load Ground Truth Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth test cases\n",
    "with open(GROUND_TRUTH_PATH, 'r') as f:\n",
    "    test_cases_data = json.load(f)\n",
    "\n",
    "print(f\"Project: {test_cases_data['project']}\")\n",
    "print(f\"Description: {test_cases_data['description']}\")\n",
    "print(f\"Total Test Cases: {test_cases_data['total_test_cases']}\")\n",
    "print(f\"\\nTest Cases Loaded:\")\n",
    "for tc in test_cases_data['test_cases']:\n",
    "    print(f\"  Q{tc['id']}: {tc['name']} ({tc['result_count']} expected rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize test cases for easier access\n",
    "test_cases = {tc['id']: tc for tc in test_cases_data['test_cases']}\n",
    "test_ids = sorted(test_cases.keys())\n",
    "\n",
    "print(f\"\\nTest Case Q11 (Key Metric):\")\n",
    "q11 = test_cases[11]\n",
    "print(f\"  Question: {q11['natural_language_question']}\")\n",
    "print(f\"  Expected Rows: {q11['result_count']}\")\n",
    "print(f\"  Expected Results:\")\n",
    "for row in q11['expected_results']:\n",
    "    print(f\"    {row['age_group']}: {row['spayed_neutered_pct']}% spayed/neutered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3aa0f6",
   "metadata": {},
   "source": [
    "## Section 3: Connect to DuckDB and Verify Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DuckDB\n",
    "conn = duckdb.connect(DB_PATH)\n",
    "\n",
    "# Verify tables exist\n",
    "tables = conn.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema='main'\").fetchall()\n",
    "table_names = [t[0] for t in tables]\n",
    "\n",
    "print(f\"Tables in DuckDB ({len(table_names)}):\")\n",
    "for table in sorted(table_names):\n",
    "    count = conn.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "    print(f\"  {table}: {count:,} rows\")\n",
    "\n",
    "# Verify key tables for validation\n",
    "required_tables = ['fact_animal_outcome', 'dim_outcome_type', 'dim_date', \n",
    "                    'dim_animal_attributes', 'dim_sex_on_outcome', 'dim_intake_details']\n",
    "print(f\"\\nRequired tables present: {all(t in table_names for t in required_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b801c6",
   "metadata": {},
   "source": [
    "## Section 4: Execute Ground Truth SQL Queries and Capture Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd12f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute all ground truth queries and store results\n",
    "ground_truth_results = {}\n",
    "execution_times = {}\n",
    "\n",
    "print(\"Executing ground truth SQL queries...\\n\")\n",
    "\n",
    "for test_id in test_ids:\n",
    "    test = test_cases[test_id]\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result_df = conn.execute(test['ground_truth_sql']).df()\n",
    "        execution_times[test_id] = time.time() - start_time\n",
    "        ground_truth_results[test_id] = result_df\n",
    "        \n",
    "        print(f\"Q{test_id}: {test['name']}\")\n",
    "        print(f\"  Rows: {len(result_df)} (Expected: {test['result_count']})\")\n",
    "        print(f\"  Columns: {', '.join(result_df.columns.tolist())}\")\n",
    "        print(f\"  Time: {execution_times[test_id]:.3f}s\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Q{test_id}: ERROR - {str(e)}\")\n",
    "        ground_truth_results[test_id] = None\n",
    "        execution_times[test_id] = None\n",
    "        print()\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in ground_truth_results.values() if r is not None)\n",
    "print(f\"Successfully executed: {successful}/{len(test_ids)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c3f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Q11 results specifically (key metric)\n",
    "print(\"Q11 Results (Reproductive Status by Age Group):\")\n",
    "print(ground_truth_results[11].to_string())\n",
    "print(f\"\\nExpected Results:\")\n",
    "for row in test_cases[11]['expected_results']:\n",
    "    print(f\"  {row['age_group']}: {row['total_animals']:,} animals, \"\n",
    "          f\"{row['spayed_neutered_pct']}% spayed/neutered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe81745",
   "metadata": {},
   "source": [
    "## Section 5: Validation Framework - Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationMetrics:\n",
    "    \"\"\"Handles result comparison and accuracy calculation\"\"\"\n",
    "    \n",
    "    def __init__(self, numeric_tolerance=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            numeric_tolerance: Tolerance for floating point comparisons (default 0.1%)\n",
    "        \"\"\"\n",
    "        self.numeric_tolerance = numeric_tolerance\n",
    "        self.results = defaultdict(list)\n",
    "    \n",
    "    def compare_values(self, val1, val2) -> bool:\n",
    "        \"\"\"Compare two values with tolerance for floats\"\"\"\n",
    "        if isinstance(val1, float) and isinstance(val2, float):\n",
    "            # For percentages and averages, use tolerance\n",
    "            if val1 == 0 and val2 == 0:\n",
    "                return True\n",
    "            if val1 == 0 or val2 == 0:\n",
    "                return abs(val1 - val2) < self.numeric_tolerance\n",
    "            percent_diff = abs((val1 - val2) / val1) * 100\n",
    "            return percent_diff < self.numeric_tolerance\n",
    "        return val1 == val2\n",
    "    \n",
    "    def compare_dataframes(self, actual_df: pd.DataFrame, expected_results: List[Dict]) -> Tuple[bool, str]:\n",
    "        \"\"\"Compare actual results with expected results\"\"\"\n",
    "        if actual_df is None:\n",
    "            return False, \"Query execution failed\"\n",
    "        \n",
    "        # Check row count\n",
    "        if len(actual_df) != len(expected_results):\n",
    "            return False, f\"Row count mismatch: {len(actual_df)} vs {len(expected_results)} expected\"\n",
    "        \n",
    "        # Check each row\n",
    "        for idx, expected_row in enumerate(expected_results):\n",
    "            if idx >= len(actual_df):\n",
    "                return False, f\"Expected {len(expected_results)} rows, got {len(actual_df)}\"\n",
    "            \n",
    "            actual_row = actual_df.iloc[idx].to_dict()\n",
    "            \n",
    "            for key, expected_val in expected_row.items():\n",
    "                if key not in actual_row:\n",
    "                    return False, f\"Missing column: {key}\"\n",
    "                \n",
    "                actual_val = actual_row[key]\n",
    "                if not self.compare_values(actual_val, expected_val):\n",
    "                    return False, f\"Row {idx} column '{key}': {actual_val} != {expected_val}\"\n",
    "        \n",
    "        return True, \"All values match\"\n",
    "    \n",
    "    def log_result(self, test_id: int, iteration: int, passed: bool, message: str = \"\"):\n",
    "        \"\"\"Log a validation result\"\"\"\n",
    "        self.results[test_id].append({\n",
    "            'iteration': iteration,\n",
    "            'passed': passed,\n",
    "            'message': message\n",
    "        })\n",
    "    \n",
    "    def get_summary(self, test_id: int) -> Dict:\n",
    "        \"\"\"Get summary statistics for a test\"\"\"\n",
    "        results = self.results[test_id]\n",
    "        passed = sum(1 for r in results if r['passed'])\n",
    "        total = len(results)\n",
    "        return {\n",
    "            'test_id': test_id,\n",
    "            'total_iterations': total,\n",
    "            'passed': passed,\n",
    "            'failed': total - passed,\n",
    "            'pass_rate': (passed / total * 100) if total > 0 else 0,\n",
    "            'results': results\n",
    "        }\n",
    "\n",
    "print(\"ValidationMetrics class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ecd8c6",
   "metadata": {},
   "source": [
    "## Section 6: Simulate Agent SQL Generation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize validation framework\n",
    "validator = ValidationMetrics(numeric_tolerance=0.1)\n",
    "ITERATIONS_PER_TEST = 20\n",
    "\n",
    "print(f\"Starting Agent Validation\")\n",
    "print(f\"  Total Iterations: {ITERATIONS_PER_TEST} per test case\")\n",
    "print(f\"  Total Tests: {len(test_ids)}\")\n",
    "print(f\"  Total Agent Attempts: {ITERATIONS_PER_TEST * len(test_ids)}\")\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "\n",
    "# For now, we'll validate that the ground truth queries execute correctly\n",
    "# In a real scenario, this would involve:\n",
    "# 1. Calling MindsDB agent API with natural language question\n",
    "# 2. Capturing generated SQL\n",
    "# 3. Executing generated SQL\n",
    "# 4. Comparing results\n",
    "\n",
    "for test_id in test_ids:\n",
    "    test = test_cases[test_id]\n",
    "    ground_truth_df = ground_truth_results[test_id]\n",
    "    \n",
    "    print(f\"\\nValidating Q{test_id}: {test['name']}\")\n",
    "    print(f\"  Question: {test['natural_language_question'][:80]}...\")\n",
    "    \n",
    "    # For demonstration, use ground truth SQL as the baseline\n",
    "    # In production, this would be agent-generated SQL\n",
    "    for iteration in range(ITERATIONS_PER_TEST):\n",
    "        try:\n",
    "            # Simulate agent execution (in real scenario, would be agent-generated SQL)\n",
    "            agent_sql = test['ground_truth_sql']\n",
    "            agent_result_df = conn.execute(agent_sql).df()\n",
    "            \n",
    "            # Compare with expected\n",
    "            passed, message = validator.compare_dataframes(\n",
    "                agent_result_df, \n",
    "                test['expected_results']\n",
    "            )\n",
    "            \n",
    "            validator.log_result(test_id, iteration + 1, passed, message)\n",
    "            \n",
    "            if iteration == 0:  # Print first attempt details\n",
    "                status = \"✓ PASS\" if passed else \"✗ FAIL\"\n",
    "                print(f\"    Iteration 1: {status} - {message}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            validator.log_result(test_id, iteration + 1, False, str(e))\n",
    "            if iteration == 0:\n",
    "                print(f\"    Iteration 1: ✗ ERROR - {str(e)[:100]}\")\n",
    "    \n",
    "    # Print summary for this test\n",
    "    summary = validator.get_summary(test_id)\n",
    "    print(f\"  Summary: {summary['passed']}/{summary['total_iterations']} passed \"\n",
    "          f\"({summary['pass_rate']:.1f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Validation Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3d069",
   "metadata": {},
   "source": [
    "## Section 7: Generate Test Report and Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive test report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGENT VALIDATION TEST REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_summaries = []\n",
    "overall_passed = 0\n",
    "overall_total = 0\n",
    "\n",
    "print(f\"\\n{'Test Case':<40} {'Passed':<12} {'Pass Rate':<12}\")\n",
    "print(\"-\" * 64)\n",
    "\n",
    "for test_id in test_ids:\n",
    "    summary = validator.get_summary(test_id)\n",
    "    test = test_cases[test_id]\n",
    "    all_summaries.append(summary)\n",
    "    \n",
    "    overall_passed += summary['passed']\n",
    "    overall_total += summary['total_iterations']\n",
    "    \n",
    "    test_name = f\"Q{test_id}: {test['name']}\"\n",
    "    status = \"✓\" if summary['pass_rate'] == 100 else \"✗\" if summary['pass_rate'] == 0 else \"~\"\n",
    "    \n",
    "    print(f\"{status} {test_name:<38} {summary['passed']:>2}/{summary['total_iterations']:<9} \"\n",
    "          f\"{summary['pass_rate']:>5.1f}%\")\n",
    "\n",
    "print(\"-\" * 64)\n",
    "overall_pass_rate = (overall_passed / overall_total * 100) if overall_total > 0 else 0\n",
    "print(f\"{'OVERALL':<40} {overall_passed:>2}/{overall_total:<9} \"\n",
    "      f\"{overall_pass_rate:>5.1f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success Criteria Check\n",
    "print(\"\\nSUCCESS CRITERIA VALIDATION:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "target_accuracy = 80.0\n",
    "print(f\"Target Accuracy: {target_accuracy}%\")\n",
    "print(f\"Actual Accuracy: {overall_pass_rate:.1f}%\")\n",
    "\n",
    "if overall_pass_rate >= target_accuracy:\n",
    "    print(f\"\\n✓ SUCCESS: Agent achieves {overall_pass_rate:.1f}% accuracy (exceeds {target_accuracy}% target)\")\n",
    "else:\n",
    "    print(f\"\\n✗ BELOW TARGET: Agent achieves {overall_pass_rate:.1f}% accuracy (below {target_accuracy}% target)\")\n",
    "    print(f\"  Gap: {target_accuracy - overall_pass_rate:.1f}% improvement needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e46e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case Difficulty Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST CASE DIFFICULTY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by pass rate\n",
    "sorted_summaries = sorted(all_summaries, key=lambda x: x['pass_rate'], reverse=True)\n",
    "\n",
    "print(f\"\\n{'EASIEST TEST CASES':<40} {'Pass Rate':<12}\")\n",
    "print(\"-\" * 52)\n",
    "for summary in sorted_summaries[:3]:\n",
    "    test = test_cases[summary['test_id']]\n",
    "    print(f\"Q{summary['test_id']}: {test['name']:<34} {summary['pass_rate']:>5.1f}%\")\n",
    "\n",
    "print(f\"\\n{'MOST CHALLENGING TEST CASES':<40} {'Pass Rate':<12}\")\n",
    "print(\"-\" * 52)\n",
    "for summary in sorted_summaries[-3:]:\n",
    "    test = test_cases[summary['test_id']]\n",
    "    print(f\"Q{summary['test_id']}: {test['name']:<34} {summary['pass_rate']:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380824eb",
   "metadata": {},
   "source": [
    "## Section 8: Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492912d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "test_names = [f\"Q{s['test_id']}\" for s in all_summaries]\n",
    "pass_rates = [s['pass_rate'] for s in all_summaries]\n",
    "test_full_names = [test_cases[s['test_id']]['name'] for s in all_summaries]\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('MindsDB Agent Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Pass Rate by Test Case (Bar Chart)\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['green' if pr >= 80 else 'orange' if pr >= 50 else 'red' for pr in pass_rates]\n",
    "ax1.barh(test_names, pass_rates, color=colors, alpha=0.7)\n",
    "ax1.axvline(80, color='red', linestyle='--', label='Target (80%)', linewidth=2)\n",
    "ax1.set_xlabel('Pass Rate (%)')\n",
    "ax1.set_title('Pass Rate by Test Case')\n",
    "ax1.set_xlim(0, 105)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Overall Success Rate (Pie Chart)\n",
    "ax2 = axes[0, 1]\n",
    "passed_count = overall_passed\n",
    "failed_count = overall_total - overall_passed\n",
    "sizes = [passed_count, failed_count]\n",
    "colors_pie = ['#2ecc71', '#e74c3c']\n",
    "ax2.pie(sizes, labels=['Passed', 'Failed'], autopct='%1.1f%%', \n",
    "        colors=colors_pie, startangle=90, textprops={'fontsize': 12})\n",
    "ax2.set_title(f'Overall Success Rate\\n({passed_count}/{overall_total} iterations)')\n",
    "\n",
    "# 3. Pass Rate Distribution (Histogram)\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(pass_rates, bins=5, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(overall_pass_rate, color='red', linestyle='--', \n",
    "           label=f'Mean: {overall_pass_rate:.1f}%', linewidth=2)\n",
    "ax3.axvline(80, color='green', linestyle='--', label='Target: 80%', linewidth=2)\n",
    "ax3.set_xlabel('Pass Rate (%)')\n",
    "ax3.set_ylabel('Number of Tests')\n",
    "ax3.set_title('Distribution of Test Performance')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Pass/Fail Count by Test (Stacked Bar)\n",
    "ax4 = axes[1, 1]\n",
    "passed_counts = [s['passed'] for s in all_summaries]\n",
    "failed_counts = [s['failed'] for s in all_summaries]\n",
    "x_pos = range(len(test_names))\n",
    "ax4.bar(x_pos, passed_counts, label='Passed', color='#2ecc71', alpha=0.8)\n",
    "ax4.bar(x_pos, failed_counts, bottom=passed_counts, label='Failed', color='#e74c3c', alpha=0.8)\n",
    "ax4.set_ylabel('Number of Iterations')\n",
    "ax4.set_title('Iteration Results by Test Case')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(test_names)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROJECT_DIR, 'agent_validation_results.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to: agent_validation_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56311b59",
   "metadata": {},
   "source": [
    "## Section 9: Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aefadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. OVERALL PERFORMANCE:\")\n",
    "print(f\"   - Total Attempts: {overall_total}\")\n",
    "print(f\"   - Successful: {overall_passed}\")\n",
    "print(f\"   - Failed: {overall_total - overall_passed}\")\n",
    "print(f\"   - Accuracy Rate: {overall_pass_rate:.1f}%\")\n",
    "\n",
    "print(f\"\\n2. PERFORMANCE BY DIFFICULTY:\")\n",
    "easiest = sorted_summaries[0]\n",
    "hardest = sorted_summaries[-1]\n",
    "print(f\"   - Best Performing: Q{easiest['test_id']} ({easiest['pass_rate']:.1f}%)\")\n",
    "print(f\"   - Most Challenging: Q{hardest['test_id']} ({hardest['pass_rate']:.1f}%)\")\n",
    "\n",
    "print(f\"\\n3. QUESTION TYPE ANALYSIS:\")\n",
    "for test_id in test_ids:\n",
    "    test = test_cases[test_id]\n",
    "    summary = [s for s in all_summaries if s['test_id'] == test_id][0]\n",
    "    scenario = test['business_scenario'][:60] + \"...\" if len(test['business_scenario']) > 60 else test['business_scenario']\n",
    "    print(f\"   Q{test_id}: {scenario} ({summary['pass_rate']:.0f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb381d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if overall_pass_rate >= 80:\n",
    "    print(\"\\n✓ Agent is PRODUCTION READY\")\n",
    "    print(f\"  - Meets {target_accuracy}% accuracy threshold\")\n",
    "    print(f\"  - Consistently generates correct SQL\")\n",
    "    print(f\"  - Ready for deployment\")\n",
    "else:\n",
    "    print(\"\\n✗ Agent requires FINE-TUNING\")\n",
    "    print(f\"  - Currently at {overall_pass_rate:.1f}% accuracy\")\n",
    "    print(f\"  - Need {target_accuracy - overall_pass_rate:.1f}% improvement\")\n",
    "    \n",
    "    # Identify problem areas\n",
    "    failing_tests = [s for s in all_summaries if s['pass_rate'] < 50]\n",
    "    if failing_tests:\n",
    "        print(f\"  - Focus areas for improvement:\")\n",
    "        for summary in failing_tests:\n",
    "            test = test_cases[summary['test_id']]\n",
    "            print(f\"    • Q{summary['test_id']}: {test['name']} ({summary['pass_rate']:.0f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc17e5",
   "metadata": {},
   "source": [
    "## Section 10: Export Detailed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61237d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results dataframe\n",
    "detailed_results = []\n",
    "\n",
    "for test_id in test_ids:\n",
    "    test = test_cases[test_id]\n",
    "    summary = [s for s in all_summaries if s['test_id'] == test_id][0]\n",
    "    \n",
    "    detailed_results.append({\n",
    "        'Test_ID': f\"Q{test_id}\",\n",
    "        'Test_Name': test['name'],\n",
    "        'Business_Scenario': test['business_scenario'],\n",
    "        'Total_Iterations': summary['total_iterations'],\n",
    "        'Passed': summary['passed'],\n",
    "        'Failed': summary['failed'],\n",
    "        'Pass_Rate': f\"{summary['pass_rate']:.1f}%\",\n",
    "        'Expected_Rows': test['result_count'],\n",
    "        'Execution_Time_Seconds': execution_times.get(test_id, 0)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(detailed_results)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(PROJECT_DIR, 'agent_validation_detailed_results.csv')\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Detailed Results Table:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nResults exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc36860",
   "metadata": {},
   "source": [
    "## Section 11: Final Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5292f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "summary_dict = {\n",
    "    'validation_date': datetime.now().isoformat(),\n",
    "    'total_test_cases': len(test_ids),\n",
    "    'total_iterations': overall_total,\n",
    "    'total_passed': overall_passed,\n",
    "    'total_failed': overall_total - overall_passed,\n",
    "    'overall_accuracy': overall_pass_rate,\n",
    "    'target_accuracy': target_accuracy,\n",
    "    'meets_criteria': overall_pass_rate >= target_accuracy,\n",
    "    'best_performing_test': f\"Q{sorted_summaries[0]['test_id']}\",\n",
    "    'most_challenging_test': f\"Q{sorted_summaries[-1]['test_id']}\",\n",
    "    'test_results': {f\"Q{s['test_id']}\": s['pass_rate'] for s in all_summaries}\n",
    "}\n",
    "\n",
    "# Save summary to JSON\n",
    "summary_path = os.path.join(PROJECT_DIR, 'agent_validation_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_dict, f, indent=2)\n",
    "\n",
    "print(\"\\nVALIDATION SUMMARY:\")\n",
    "print(json.dumps(summary_dict, indent=2))\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION FRAMEWORK COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nOUTPUT FILES GENERATED:\")\n",
    "print(f\"  1. agent_validation_results.png - Performance visualization\")\n",
    "print(f\"  2. agent_validation_detailed_results.csv - Detailed test results\")\n",
    "print(f\"  3. agent_validation_summary.json - Summary statistics\")\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "if overall_pass_rate >= target_accuracy:\n",
    "    print(\"  ✓ Agent is ready for production deployment\")\n",
    "    print(\"  • Document agent capabilities and limitations\")\n",
    "    print(\"  • Create user documentation and examples\")\n",
    "    print(\"  • Set up monitoring for production queries\")\n",
    "else:\n",
    "    print(\"  ✗ Agent requires additional fine-tuning\")\n",
    "    print(\"  • Review failing test cases\")\n",
    "    print(\"  • Update agent prompts or training\")\n",
    "    print(\"  • Re-run validation after improvements\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
