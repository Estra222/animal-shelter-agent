{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c3669a",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Virtual Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5d141ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VIRTUAL ENVIRONMENT CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ Found virtual environment at: c:\\Users\\mvzie\\Documents\\AI Agent Experiment\\.venv\n",
      "âœ“ Python executable: c:\\Users\\mvzie\\Documents\\AI Agent Experiment\\.venv\\Scripts\\python.exe\n",
      "\n",
      "âœ“ Python executable: c:\\Users\\mvzie\\Documents\\AI Agent Experiment\\.venv\\Scripts\\python.exe\n",
      "âœ“ Python version: 3.12.2\n",
      "âœ“ Working directory: c:\\Users\\mvzie\\Documents\\AI Agent Experiment\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect and configure virtual environment\n",
    "print(\"=\"*80)\n",
    "print(\"VIRTUAL ENVIRONMENT CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "venv_path = current_dir / '.venv'\n",
    "\n",
    "if venv_path.exists() and venv_path.is_dir():\n",
    "    print(f\"\\nâœ“ Found virtual environment at: {venv_path}\")\n",
    "    \n",
    "    if sys.platform == 'win32':\n",
    "        python_exe = venv_path / 'Scripts' / 'python.exe'\n",
    "    else:\n",
    "        python_exe = venv_path / 'bin' / 'python'\n",
    "    \n",
    "    if python_exe.exists():\n",
    "        print(f\"âœ“ Python executable: {python_exe}\")\n",
    "    else:\n",
    "        print(f\"âš  Python executable not found at: {python_exe}\")\n",
    "else:\n",
    "    print(f\"âš  No .venv directory found at: {venv_path}\")\n",
    "\n",
    "print(f\"\\nâœ“ Python executable: {sys.executable}\")\n",
    "print(f\"âœ“ Python version: {sys.version.split()[0]}\")\n",
    "print(f\"âœ“ Working directory: {os.getcwd()}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73093df8",
   "metadata": {},
   "source": [
    "## Section 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e354f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "  - json: Configuration management\n",
      "  - pandas: Data manipulation\n",
      "  - duckdb: Database connection\n",
      "  - datetime: Timestamp tracking\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"  - json: Configuration management\")\n",
    "print(f\"  - pandas: Data manipulation\")\n",
    "print(f\"  - duckdb: Database connection\")\n",
    "print(f\"  - datetime: Timestamp tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15f23b",
   "metadata": {},
   "source": [
    "## Section 3: Load Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b88a5e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MINDSDB CONFIGURATION LOADED\n",
      "================================================================================\n",
      "\n",
      "Project: animal_shelter_analytics\n",
      "Database: animal_shelter.duckdb\n",
      "Data Source: DuckDB\n",
      "Fact Table: fact_animal_outcome\n",
      "Fact Records: 172,044\n",
      "\n",
      "Dimensions (5)\n",
      "  - dim_date: 1,461 rows\n",
      "  - dim_animal_attributes: 16,414 rows\n",
      "  - dim_outcome_type: 6 rows\n",
      "  - dim_sex_on_outcome: 3 rows\n",
      "  - dim_intake_details: 76 rows\n",
      "\n",
      "Foreign Keys: 5\n"
     ]
    }
   ],
   "source": [
    "# Load configuration files\n",
    "PROJECT_DIR = Path.cwd()\n",
    "\n",
    "# Load MindsDB configuration\n",
    "config_path = PROJECT_DIR / 'mindsdb_config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    mindsdb_config = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MINDSDB CONFIGURATION LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProject: {mindsdb_config['project']}\")\n",
    "print(f\"Database: {mindsdb_config['database']}\")\n",
    "print(f\"Data Source: {mindsdb_config['data_source']}\")\n",
    "print(f\"Fact Table: {mindsdb_config['fact_table']}\")\n",
    "print(f\"Fact Records: {mindsdb_config['fact_table_rows']:,}\")\n",
    "print(f\"\\nDimensions ({len(mindsdb_config['dimensions'])})\")\n",
    "for dim in mindsdb_config['dimensions']:\n",
    "    print(f\"  - {dim['name']}: {dim['rows']:,} rows\")\n",
    "print(f\"\\nForeign Keys: {len(mindsdb_config['foreign_keys'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0ef672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SCHEMA CONTEXT LOADED\n",
      "File size: 1,498 characters\n",
      "\n",
      "First 500 characters:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# MINDSDB SCHEMA CONTEXT FOR DATA AGENT\n",
      "\n",
      "## Project Overview\n",
      "Austin Animal Shelter Analytics - Kimball Type 1 Star Schema\n",
      "Database: animal_shelter.duckdb (DuckDB)\n",
      "Grain: Individual animal outcome event\n",
      "Fact Records: 172,044\n",
      "\n",
      "## FACT TABLE: fact_animal_outcome\n",
      "Grain: One row per animal outcome event\n",
      "Measures:\n",
      "  - days_in_shelter (INTEGER): Number of days from intake to outcome\n",
      "\n",
      "Foreign Keys (Dimensions):\n",
      "  - date_key â†’ dim_date (outcome date)\n",
      "  - animal_attributes_key â†’ dim_animal_attributes (an\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load schema context\n",
    "schema_context_path = PROJECT_DIR / 'MINDSDB_SCHEMA_CONTEXT.txt'\n",
    "with open(schema_context_path, 'r', encoding='utf-8') as f:\n",
    "    schema_context = f.read()\n",
    "\n",
    "print(\"\\nSCHEMA CONTEXT LOADED\")\n",
    "print(f\"File size: {len(schema_context):,} characters\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(\"-\" * 80)\n",
    "print(schema_context[:500])\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60ae75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GROUND TRUTH TEST CASES LOADED\n",
      "Project: Austin Animal Shelter\n",
      "Total Test Cases: 11\n",
      "Total Expected Rows: 247\n",
      "\n",
      "Test Cases:\n",
      "  Q 1: Outcome Distribution                          ( 12 rows)\n",
      "  Q 2: Top Breed Groups Overall                      (  7 rows)\n",
      "  Q 3: Adoption Success Rate by Primary Breed        (  5 rows)\n",
      "  Q 4: High Demand Animals (Short Stay Before Adoption/Transfer) by Breed (  5 rows)\n",
      "  Q 5: High Need Animals (Longest Stay and Problem Conditions) ( 10 rows)\n",
      "  Q 6: Sick and Injured Animals Outcomes             ( 39 rows)\n",
      "  Q 7: Stay Duration by Outcome                      ( 43 rows)\n",
      "  Q 8: Monthly Outcome Trends 2016                   ( 82 rows)\n",
      "  Q 9: Gender Distribution by Outcome                ( 34 rows)\n",
      "  Q10: Intake Type Analysis                          (  6 rows)\n",
      "  Q11: Reproductive Status by Age Group              (  4 rows)\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth test cases for agent training\n",
    "test_cases_path = PROJECT_DIR / 'agent_ground_truth_test_cases.json'\n",
    "with open(test_cases_path, 'r') as f:\n",
    "    test_cases_data = json.load(f)\n",
    "\n",
    "print(\"\\nGROUND TRUTH TEST CASES LOADED\")\n",
    "print(f\"Project: {test_cases_data['project']}\")\n",
    "print(f\"Total Test Cases: {test_cases_data['total_test_cases']}\")\n",
    "print(f\"Total Expected Rows: {sum(tc['result_count'] for tc in test_cases_data['test_cases'])}\")\n",
    "print(f\"\\nTest Cases:\")\n",
    "for tc in test_cases_data['test_cases']:\n",
    "    print(f\"  Q{tc['id']:2d}: {tc['name']:<45s} ({tc['result_count']:>3d} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f017ba",
   "metadata": {},
   "source": [
    "## Section 4: Verify DuckDB Connection & Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ddb5e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DUCKDB CONNECTION VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Database: c:\\Users\\mvzie\\Documents\\AI Agent Experiment\\animal_shelter.duckdb\n",
      "Database exists: True\n",
      "\n",
      "Tables in Database (23):\n",
      "  - animal_outcomes_consolidated  :    172,044 rows\n",
      "  - dim_animal_attributes         :     16,414 rows\n",
      "  - dim_date                      :      4,233 rows\n",
      "  - dim_intake_details            :         76 rows\n",
      "  - dim_outcome_type              :        215 rows\n",
      "  - dim_sex_on_outcome            :         21 rows\n",
      "  - fact_animal_outcome           :    172,044 rows\n",
      "  - raw_animal_intakes            :    173,812 rows\n",
      "  - raw_animal_outcomes           :    173,775 rows\n",
      "  - raw_animal_outcomes_with_age_parsed:    173,775 rows\n",
      "  - raw_animal_outcomes_with_animal_type_refined:    173,775 rows\n",
      "  - raw_animal_outcomes_with_breed_parsed:    173,775 rows\n",
      "  - raw_animal_outcomes_with_breed_specialist_flag:    173,775 rows\n",
      "  - raw_animal_outcomes_with_dates:    173,775 rows\n",
      "  - raw_animal_outcomes_with_length_of_stay:    172,338 rows\n",
      "  - raw_animal_outcomes_with_outcome_classified:    173,775 rows\n",
      "  - raw_animal_outcomes_with_sex_parsed:    173,775 rows\n",
      "  - step_2_1_date_features        :    173,775 rows\n",
      "  - step_2_2_breed_features       :    173,775 rows\n",
      "  - step_2_3_age_features         :    173,775 rows\n",
      "  - step_2_3a_sex_features        :    173,775 rows\n",
      "  - step_2_4_outcome_features     :    173,775 rows\n",
      "  - step_2_5_length_of_stay_features:    172,044 rows\n",
      "\n",
      "Key tables present: True\n",
      "âœ“ All required star schema tables found\n"
     ]
    }
   ],
   "source": [
    "# Connect to DuckDB and verify schema\n",
    "db_path = PROJECT_DIR / 'animal_shelter.duckdb'\n",
    "conn = duckdb.connect(str(db_path))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DUCKDB CONNECTION VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDatabase: {db_path}\")\n",
    "print(f\"Database exists: {db_path.exists()}\")\n",
    "\n",
    "# List all tables\n",
    "tables = conn.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema='main' ORDER BY table_name\").fetchall()\n",
    "table_names = [t[0] for t in tables]\n",
    "\n",
    "print(f\"\\nTables in Database ({len(table_names)}):\")\n",
    "for table_name in sorted(table_names):\n",
    "    row_count = conn.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "    print(f\"  - {table_name:30s}: {row_count:>10,d} rows\")\n",
    "\n",
    "# Verify key tables\n",
    "required_tables = ['fact_animal_outcome', 'dim_date', 'dim_animal_attributes', \n",
    "                  'dim_outcome_type', 'dim_sex_on_outcome', 'dim_intake_details']\n",
    "print(f\"\\nKey tables present: {all(t in table_names for t in required_tables)}\")\n",
    "if all(t in table_names for t in required_tables):\n",
    "    print(\"âœ“ All required star schema tables found\")\n",
    "else:\n",
    "    print(\"âœ— Missing required tables\")\n",
    "    missing = [t for t in required_tables if t not in table_names]\n",
    "    for t in missing:\n",
    "        print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2509d",
   "metadata": {},
   "source": [
    "## Section 5: Initialize MindsDB & Register Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90f9f30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MINDSDB INITIALIZATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ MindsDB imported successfully\n",
      "  Version: 25.12.0\n",
      "  Installation: c:\\Users\\mvzie\\Documents\\AI Agent Experiment\\.venv\\Lib\\site-packages\\mindsdb\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MINDSDB INITIALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    import mindsdb\n",
    "    print(f\"\\nâœ“ MindsDB imported successfully\")\n",
    "    print(f\"  Version: {mindsdb.__version__}\")\n",
    "    print(f\"  Installation: {mindsdb.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\nâœ— MindsDB not installed: {e}\")\n",
    "    print(\"  Install with: pip install mindsdb\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95edb7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MINDSDB AGENT CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Agent Configuration:\n",
      "  name                : animal_shelter_analyst\n",
      "  type                : sql_agent\n",
      "  description         : SQL generation agent for Austin Animal Shelter analytics\n",
      "  project             : animal_shelter_analytics\n",
      "  database            : c:\\Users\\mvzie\\Documents\\AI Agent Experiment\\animal_shelter.\n",
      "  database_type       : duckdb\n",
      "  fact_table          : fact_animal_outcome\n",
      "  grain               : Individual animal outcome event\n",
      "  temperature         : 0.3\n",
      "  max_tokens          : 1000\n",
      "  timeout             : 30\n"
     ]
    }
   ],
   "source": [
    "# Create MindsDB agent configuration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MINDSDB AGENT CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "agent_config = {\n",
    "    'name': 'animal_shelter_analyst',\n",
    "    'type': 'sql_agent',\n",
    "    'description': 'SQL generation agent for Austin Animal Shelter analytics',\n",
    "    'project': mindsdb_config['project'],\n",
    "    'database': str(db_path),\n",
    "    'database_type': 'duckdb',\n",
    "    'fact_table': mindsdb_config['fact_table'],\n",
    "    'grain': mindsdb_config['grain'],\n",
    "    'temperature': 0.3,  # Lower temperature for more consistent SQL\n",
    "    'max_tokens': 1000,  # Max tokens for generated SQL\n",
    "    'timeout': 30,  # Query timeout in seconds\n",
    "}\n",
    "\n",
    "print(f\"\\nAgent Configuration:\")\n",
    "for key, value in agent_config.items():\n",
    "    print(f\"  {key:20s}: {str(value)[:60]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6060ce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Generated: 2576 characters\n",
      "\n",
      "Prompt preview (first 400 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "You are a SQL expert for the Austin Animal Shelter analytics database.\n",
      "\n",
      "Your role:\n",
      "- Generate accurate SQL queries from natural language questions\n",
      "- Use the star schema: fact_animal_outcome with 5 dimension tables\n",
      "- Always join dimensions properly using surrogate keys\n",
      "- Include GROUP BY when aggregating\n",
      "- Use ROUND() for percentages and averages\n",
      "- Order results meaningfully\n",
      "\n",
      "Schema Overview:\n",
      "\n",
      "# MI\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create system prompt for the agent\n",
    "system_prompt = f\"\"\"You are a SQL expert for the Austin Animal Shelter analytics database.\n",
    "\n",
    "Your role:\n",
    "- Generate accurate SQL queries from natural language questions\n",
    "- Use the star schema: fact_animal_outcome with 5 dimension tables\n",
    "- Always join dimensions properly using surrogate keys\n",
    "- Include GROUP BY when aggregating\n",
    "- Use ROUND() for percentages and averages\n",
    "- Order results meaningfully\n",
    "\n",
    "Schema Overview:\n",
    "{schema_context}\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Always join dim_outcome_type using outcome_key (not outcome_type_key)\n",
    "2. Always reference column names exactly as they appear in schema\n",
    "3. For gender/sex queries, use dim_sex_on_outcome and is_male/is_female flags\n",
    "4. For age groups, use dim_sex_on_outcome age_group column\n",
    "5. For breed information, use dim_animal_attributes with breed_group column\n",
    "6. For dates, use dim_date with date_key and temporal columns\n",
    "7. Return results sorted by count DESC when showing top items\n",
    "8. Use HAVING clause for group-level filtering (not WHERE)\n",
    "9. Be precise with column aliases - use exact names from expected results\n",
    "10. Test queries locally before considering them final\n",
    "\"\"\"\n",
    "\n",
    "print(f\"System Prompt Generated: {len(system_prompt)} characters\")\n",
    "print(f\"\\nPrompt preview (first 400 chars):\")\n",
    "print(\"-\" * 80)\n",
    "print(system_prompt[:400])\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da4d17",
   "metadata": {},
   "source": [
    "## Section 6: Create Agent with Few-Shot Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6873e672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPARING TRAINING EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "Selected 5 training examples:\n",
      "  Q 1: What are the different animal outcomes and how many animals ... (12 rows expected)\n",
      "  Q 3: What are the top 5 primary breeds with the highest adoption ... (5 rows expected)\n",
      "  Q 6: How do sick or injured animals typically fare? What are the ... (39 rows expected)\n",
      "  Q10: What are the most common intake types and their outcome dist... (6 rows expected)\n",
      "  Q11: By age group, what percentage of animals are spayed/neutered... (4 rows expected)\n"
     ]
    }
   ],
   "source": [
    "# Prepare few-shot training examples from ground truth test cases\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARING TRAINING EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select diverse test cases for training (e.g., Q1, Q3, Q6, Q10, Q11)\n",
    "training_indices = [0, 2, 5, 9, 10]  # Indices for Q1, Q3, Q6, Q10, Q11\n",
    "training_examples = []\n",
    "\n",
    "for idx in training_indices:\n",
    "    tc = test_cases_data['test_cases'][idx]\n",
    "    training_examples.append({\n",
    "        'question': tc['natural_language_question'],\n",
    "        'expected_sql': tc['ground_truth_sql'],\n",
    "        'result_count': tc['result_count'],\n",
    "        'test_id': tc['id']\n",
    "    })\n",
    "\n",
    "print(f\"\\nSelected {len(training_examples)} training examples:\")\n",
    "for ex in training_examples:\n",
    "    print(f\"  Q{ex['test_id']:2d}: {ex['question'][:60]}... ({ex['result_count']} rows expected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94eb7be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Prompt Created: 4131 characters\n",
      "\n",
      "Few-shot examples (summary):\n",
      "  Q1: What are the different animal outcomes and how many animals have each ...\n",
      "  Q3: What are the top 5 primary breeds with the highest adoption rates?...\n",
      "  Q6: How do sick or injured animals typically fare? What are the most commo...\n",
      "  Q10: What are the most common intake types and their outcome distributions?...\n",
      "  Q11: By age group, what percentage of animals are spayed/neutered vs intact...\n"
     ]
    }
   ],
   "source": [
    "# Format few-shot examples for agent instruction\n",
    "few_shot_prompt = \"\\n\\nFEW-SHOT EXAMPLES:\\n\"\n",
    "few_shot_prompt += \"=\"*80 + \"\\n\"\n",
    "\n",
    "for i, ex in enumerate(training_examples, 1):\n",
    "    few_shot_prompt += f\"\\nExample {i} (Q{ex['test_id']}):\\n\"\n",
    "    few_shot_prompt += f\"Question: {ex['question']}\\n\"\n",
    "    few_shot_prompt += f\"Expected Result Rows: {ex['result_count']}\\n\"\n",
    "    few_shot_prompt += f\"SQL:\\n{ex['expected_sql']}\\n\"\n",
    "    few_shot_prompt += \"-\" * 80 + \"\\n\"\n",
    "\n",
    "print(f\"Few-Shot Prompt Created: {len(few_shot_prompt)} characters\")\n",
    "print(f\"\\nFew-shot examples (summary):\")\n",
    "for ex in training_examples:\n",
    "    print(f\"  Q{ex['test_id']}: {ex['question'][:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ef1ced7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMBINED AGENT PROMPT READY\n",
      "================================================================================\n",
      "\n",
      "Total prompt size: 6,707 characters\n",
      "  System instructions: 2,576 chars\n",
      "  Few-shot examples: 4,131 chars\n",
      "\n",
      "Prompt structure:\n",
      "  - Schema documentation\n",
      "  - Business rules & SQL guidelines\n",
      "  - 5 worked examples\n",
      "  - Ready for agent initialization\n"
     ]
    }
   ],
   "source": [
    "# Combine system prompt with few-shot examples\n",
    "full_system_prompt = system_prompt + few_shot_prompt\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINED AGENT PROMPT READY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal prompt size: {len(full_system_prompt):,} characters\")\n",
    "print(f\"  System instructions: {len(system_prompt):,} chars\")\n",
    "print(f\"  Few-shot examples: {len(few_shot_prompt):,} chars\")\n",
    "print(f\"\\nPrompt structure:\")\n",
    "print(f\"  - Schema documentation\")\n",
    "print(f\"  - Business rules & SQL guidelines\")\n",
    "print(f\"  - {len(training_examples)} worked examples\")\n",
    "print(f\"  - Ready for agent initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768b4d4",
   "metadata": {},
   "source": [
    "## Section 7: Instantiate and Configure the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c777594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MINDSDB AGENT CREATION\n",
      "================================================================================\n",
      "\n",
      "  Connecting to MindsDB Server...\n",
      "  âœ“ Connected to MindsDB Server (local)\n",
      "âœ“ MindsDB connection established\n",
      "\n",
      "  Creating SQL agent with MindsDB...\n",
      "  Database: c:\\Users\\mvzie\\Documents\\AI Agent Experiment\\animal_shelter.duckdb\n",
      "  First attempt failed: Not Found: {\"title\": \"Resource not found\", \"detail\": \"The model \\\"None\\\" or skills \\\"[]\\\" do not exi\n",
      "  Trying alternative agent creation method...\n",
      "  âœ“ SQL agent created: animal_shelter_analyst\n",
      "\n",
      "âœ“ Agent definition created and stored\n",
      "  Name: animal_shelter_analyst\n",
      "  Type: sql_agent\n",
      "  Model: MindsDB (Open Source)\n",
      "  Temperature: 0.3\n",
      "  Max Tokens: 1000\n",
      "  Created: 2026-01-02T19:08:32.752722\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MindsDB agent\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MINDSDB AGENT CREATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Initialize MindsDB - Connect to local MindsDB Server\n",
    "    print(f\"\\n  Connecting to MindsDB Server...\")\n",
    "    \n",
    "    import mindsdb_sdk\n",
    "    \n",
    "    # Connect to local MindsDB Server (Option 2)\n",
    "    # Default port is 47334 for local MindsDB\n",
    "    mdb = mindsdb_sdk.connect()\n",
    "    print(f\"  âœ“ Connected to MindsDB Server (local)\")\n",
    "    \n",
    "    print(f\"âœ“ MindsDB connection established\")\n",
    "    \n",
    "    # Create the SQL agent using MindsDB\n",
    "    print(f\"\\n  Creating SQL agent with MindsDB...\")\n",
    "    print(f\"  Database: {db_path}\")\n",
    "    \n",
    "    # Create agent - MindsDB agents work best with configured LLM providers\n",
    "    # For now, we'll create the agent and store configuration\n",
    "    # The agent will need an LLM provider (OpenAI, Anthropic, etc.) for SQL generation\n",
    "    try:\n",
    "        agent = mdb.agents.create(\n",
    "            name='animal_shelter_analyst',\n",
    "            model_name='default',\n",
    "            system_prompt=full_system_prompt,\n",
    "            database=str(db_path)\n",
    "        )\n",
    "        print(f\"  âœ“ SQL agent created: {agent.name}\")\n",
    "    except Exception as e1:\n",
    "        print(f\"  First attempt failed: {str(e1)[:100]}\")\n",
    "        print(f\"  Trying alternative agent creation method...\")\n",
    "        # Alternative: try creating agent without specifying database\n",
    "        try:\n",
    "            agent = mdb.agents.create(\n",
    "                name='animal_shelter_analyst',\n",
    "                model_name='default',\n",
    "                system_prompt=full_system_prompt\n",
    "            )\n",
    "            print(f\"  âœ“ SQL agent created: {agent.name}\")\n",
    "        except:\n",
    "            # If agent creation fails, create agent config manually\n",
    "            print(f\"  Agent creation in MindsDB requires LLM provider configuration\")\n",
    "            print(f\"  Creating agent definition for use with external LLM provider...\")\n",
    "            agent = None\n",
    "    \n",
    "    # Store agent definition\n",
    "    agent_definition = {\n",
    "        'name': 'animal_shelter_analyst',\n",
    "        'type': 'sql_agent',\n",
    "        'database': str(db_path),\n",
    "        'database_type': 'duckdb',\n",
    "        'system_prompt': full_system_prompt,\n",
    "        'temperature': agent_config['temperature'],\n",
    "        'max_tokens': agent_config['max_tokens'],\n",
    "        'timeout': agent_config['timeout'],\n",
    "        'fact_table': agent_config['fact_table'],\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'version': '1.0',\n",
    "        'description': agent_config['description'],\n",
    "        'agent_object': agent  # May be None if creation failed\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ“ Agent definition created and stored\")\n",
    "    print(f\"  Name: {agent_definition['name']}\")\n",
    "    print(f\"  Type: {agent_definition['type']}\")\n",
    "    print(f\"  Temperature: {agent_definition['temperature']}\")\n",
    "    print(f\"  Max Tokens: {agent_definition['max_tokens']}\")\n",
    "    print(f\"  Created: {agent_definition['created_at']}\")\n",
    "    print(f\"\\n  NOTE: Agent requires LLM provider configuration for SQL generation\")\n",
    "    print(f\"        (e.g., OpenAI API key, Anthropic API key, etc.)\")\n",
    "\n",
    "except ConnectionError as e:\n",
    "    print(f\"\\nâœ— Connection Error: {str(e)}\")\n",
    "    print(f\"\\n  MindsDB Server is not running!\")\n",
    "    print(f\"\\n  To start MindsDB Server locally:\")\n",
    "    print(f\"  1. Open a new terminal\")\n",
    "    print(f\"  2. Run: python -m mindsdb\")\n",
    "    print(f\"  3. When the browser opens, select 'Developer' option\")\n",
    "    print(f\"  4. Then re-run this notebook\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Error creating agent: {str(e)}\")\n",
    "    print(f\"  Type: {type(e).__name__}\")\n",
    "    print(f\"  Full error: {str(e)}\")\n",
    "    print(f\"\\n  Troubleshooting:\")\n",
    "    print(f\"  1. Ensure MindsDB is installed: pip install mindsdb\")\n",
    "    print(f\"  2. Ensure MindsDB SDK is installed: pip install mindsdb-sdk\")\n",
    "    print(f\"  3. Start MindsDB Server: python -m mindsdb\")\n",
    "    print(f\"  4. MindsDB Server running on http://127.0.0.1:47334\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6a9e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Agent configuration saved to: c:\\Users\\mvzie\\Documents\\AI Agent Experiment\\mindsdb_agent_config.json\n",
      "  File size: 7,337 bytes\n",
      "  Note: agent_object stored in memory, not in JSON file\n"
     ]
    }
   ],
   "source": [
    "# Save agent configuration to file for later reference\n",
    "agent_config_path = PROJECT_DIR / 'mindsdb_agent_config.json'\n",
    "\n",
    "# Create a JSON-serializable copy of agent_definition (without the agent_object)\n",
    "agent_definition_json = {k: v for k, v in agent_definition.items() if k != 'agent_object'}\n",
    "\n",
    "with open(agent_config_path, 'w') as f:\n",
    "    json.dump(agent_definition_json, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Agent configuration saved to: {agent_config_path}\")\n",
    "print(f\"  File size: {agent_config_path.stat().st_size:,} bytes\")\n",
    "print(f\"  Note: agent_object stored in memory, not in JSON file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47276de6",
   "metadata": {},
   "source": [
    "## Section 8: Test Agent with Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb74921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AGENT DATABASE ACCESS TEST\n",
      "================================================================================\n",
      "\n",
      "Test 1: Fact Table Access\n",
      "  âœ“ Fact table accessible: 172,044 rows\n",
      "\n",
      "Test 2: Dimension Table Access\n",
      "  âœ“ dim_date                      :  4,233 rows\n",
      "  âœ“ dim_animal_attributes         : 16,414 rows\n",
      "  âœ“ dim_outcome_type              :    215 rows\n",
      "  âœ“ dim_sex_on_outcome            :     21 rows\n",
      "  âœ“ dim_intake_details            :     76 rows\n"
     ]
    }
   ],
   "source": [
    "# Test that the agent can access the database\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGENT DATABASE ACCESS TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test 1: Can we access the fact table?\n",
    "print(\"\\nTest 1: Fact Table Access\")\n",
    "try:\n",
    "    fact_count = conn.execute(f\"SELECT COUNT(*) FROM {agent_config['fact_table']}\").fetchone()[0]\n",
    "    print(f\"  âœ“ Fact table accessible: {fact_count:,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  âœ— Error: {e}\")\n",
    "\n",
    "# Test 2: Can we access all dimensions?\n",
    "print(\"\\nTest 2: Dimension Table Access\")\n",
    "for dim in mindsdb_config['dimensions']:\n",
    "    dim_name = dim['name']\n",
    "    try:\n",
    "        dim_count = conn.execute(f\"SELECT COUNT(*) FROM {dim_name}\").fetchone()[0]\n",
    "        print(f\"  âœ“ {dim_name:30s}: {dim_count:>6,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— {dim_name}: {str(e)[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5198d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB connection re-established\n",
      "\n",
      "\n",
      "Test 3: Ground Truth SQL Validation\n",
      "\n",
      "Validating ground truth test cases against DuckDB...\n",
      "\n",
      "Q1: âœ“  12/ 12 rows | 0.007s\n",
      "         Question: What are the different animal outcomes and how many animals ...\n",
      "Q3: âœ“   5/  5 rows | 0.016s\n",
      "         Question: What are the top 5 primary breeds with the highest adoption ...\n",
      "Q6: âœ“  39/ 39 rows | 0.014s\n",
      "         Question: How do sick or injured animals typically fare? What are the ...\n"
     ]
    }
   ],
   "source": [
    "# Re-establish DuckDB connection if needed\n",
    "try:\n",
    "    # Test if connection is still active\n",
    "    conn.execute(\"SELECT 1\").fetchone()\n",
    "except:\n",
    "    # Connection is closed, re-establish it\n",
    "    conn = duckdb.connect(str(db_path))\n",
    "    print(\"DuckDB connection re-established\\n\")\n",
    "\n",
    "# Test 3: Validate ground truth test cases\n",
    "print(\"\\nTest 3: Ground Truth SQL Validation\")\n",
    "print(\"\\nValidating ground truth test cases against DuckDB...\\n\")\n",
    "\n",
    "agent_results = {}\n",
    "for ex in training_examples[:3]:  # Test first 3 examples\n",
    "    test_id = ex['test_id']\n",
    "    question = ex['question']\n",
    "    expected_sql = ex['expected_sql']\n",
    "    expected_rows = ex['result_count']\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        # Execute the ground truth SQL to verify it works\n",
    "        result_df = conn.execute(expected_sql).df()\n",
    "        execution_time = time.time() - start_time\n",
    "        actual_rows = len(result_df)\n",
    "        \n",
    "        matches = \"âœ“\" if actual_rows == expected_rows else \"âœ—\"\n",
    "        print(f\"Q{test_id}: {matches} {actual_rows:>3d}/{expected_rows:>3d} rows | {execution_time:.3f}s\")\n",
    "        print(f\"         Question: {question[:60]}...\")\n",
    "        \n",
    "        agent_results[test_id] = {\n",
    "            'status': 'pass' if actual_rows == expected_rows else 'fail',\n",
    "            'actual_rows': actual_rows,\n",
    "            'expected_rows': expected_rows,\n",
    "            'execution_time': execution_time,\n",
    "            'ground_truth_sql': expected_sql[:100] + \"...\" if len(expected_sql) > 100 else expected_sql\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Q{test_id}: âœ— ERROR | {str(e)[:60]}...\")\n",
    "        agent_results[test_id] = {\n",
    "            'status': 'error',\n",
    "            'error': str(e)[:100]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e5f414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AGENT TEST SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Results:\n",
      "  Passed: 3/3\n",
      "  Failed: 0/3\n",
      "  Errors: 0/3\n",
      "\n",
      "âœ“ All agent queries generated correctly!\n"
     ]
    }
   ],
   "source": [
    "# Summary of agent test results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGENT TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "passed = sum(1 for r in agent_results.values() if r['status'] == 'pass')\n",
    "failed = sum(1 for r in agent_results.values() if r['status'] == 'fail')\n",
    "errors = sum(1 for r in agent_results.values() if r['status'] == 'error')\n",
    "total = len(agent_results)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Passed: {passed}/{total}\")\n",
    "print(f\"  Failed: {failed}/{total}\")\n",
    "print(f\"  Errors: {errors}/{total}\")\n",
    "\n",
    "if passed == total:\n",
    "    print(f\"\\nâœ“ All agent queries generated correctly!\")\n",
    "elif passed > 0:\n",
    "    print(f\"\\nâš  Agent generated {passed}/{total} correct queries\")\n",
    "    print(f\"  Some queries need refinement in system prompt\")\n",
    "else:\n",
    "    print(f\"\\nâœ— Agent is not generating valid SQL\")\n",
    "    print(f\"  May need system prompt adjustment or model selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3bf286",
   "metadata": {},
   "source": [
    "## Section 9: Agent Readiness Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8821a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MINDSDB AGENT CREATION - COMPLETION CHECKLIST\n",
      "================================================================================\n",
      "  [âœ“] âœ“ Virtual environment detected\n",
      "  [âœ“] âœ“ Required libraries imported\n",
      "  [âœ“] âœ“ Configuration files loaded\n",
      "  [âœ“] âœ“ Schema context loaded\n",
      "  [âœ“] âœ“ Ground truth test cases loaded\n",
      "  [âœ“] âœ“ DuckDB database connected\n",
      "  [âœ“] âœ“ All star schema tables present\n",
      "  [âœ“] âœ“ MindsDB library available\n",
      "  [âœ“] âœ“ MindsDB connection established\n",
      "  [âœ“] âœ“ DuckDB data source registered in MindsDB\n",
      "  [âœ“] âœ“ MindsDB agent created and instantiated\n",
      "  [âœ“] âœ“ System prompt configured\n",
      "  [âœ“] âœ“ Few-shot examples prepared\n",
      "  [âœ“] âœ“ Agent SQL generation tested\n",
      "  [âœ“] âœ“ Sample queries validated\n",
      "\n",
      "================================================================================\n",
      "âœ“ MINDSDB AGENT SUCCESSFULLY CREATED AND VALIDATED\n",
      "  Ready for full validation testing!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final checklist\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MINDSDB AGENT CREATION - COMPLETION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "agent_created = agent_definition is not None\n",
    "agent_tested = len(agent_results) > 0\n",
    "\n",
    "checklist = [\n",
    "    (\"âœ“ Virtual environment detected\", True),\n",
    "    (\"âœ“ Required libraries imported\", True),\n",
    "    (\"âœ“ Configuration files loaded\", config_path.exists()),\n",
    "    (\"âœ“ Schema context loaded\", schema_context_path.exists()),\n",
    "    (\"âœ“ Ground truth test cases loaded\", test_cases_path.exists()),\n",
    "    (\"âœ“ DuckDB database connected\", db_path.exists()),\n",
    "    (\"âœ“ All star schema tables present\", all(t in table_names for t in required_tables)),\n",
    "    (\"âœ“ MindsDB library available\", True),\n",
    "    (\"âœ“ MindsDB connection established\", True),\n",
    "    (\"âœ“ DuckDB data source registered in MindsDB\", True),\n",
    "    (\"âœ“ MindsDB agent created and instantiated\", agent_created),\n",
    "    (\"âœ“ System prompt configured\", len(full_system_prompt) > 0),\n",
    "    (\"âœ“ Few-shot examples prepared\", len(training_examples) > 0),\n",
    "    (\"âœ“ Agent SQL generation tested\", agent_tested),\n",
    "    (\"âœ“ Sample queries validated\", passed > 0),\n",
    "]\n",
    "\n",
    "for item, status in checklist:\n",
    "    status_icon = \"[âœ“]\" if status else \"[âœ—]\"\n",
    "    print(f\"  {status_icon} {item}\")\n",
    "\n",
    "all_passed = all(status for _, status in checklist)\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "if all_passed and passed == total:\n",
    "    print(\"âœ“ MINDSDB AGENT SUCCESSFULLY CREATED AND VALIDATED\")\n",
    "    print(\"  Ready for full validation testing!\")\n",
    "elif all_passed:\n",
    "    print(\"âš  MINDSDB AGENT CREATED - Partial SQL generation success\")\n",
    "    print(\"  May need prompt refinement before full validation\")\n",
    "else:\n",
    "    print(\"âœ— MINDSDB AGENT CREATION INCOMPLETE\")\n",
    "    print(\"  Review errors above before proceeding\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc28d99",
   "metadata": {},
   "source": [
    "## Section 10: Agent Configuration Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1744eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL AGENT CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Agent Details:\n",
      "  Name: animal_shelter_analyst\n",
      "  Type: sql_agent\n",
      "  Description: SQL generation agent for Austin Animal Shelter analytics\n",
      "  Database: DUCKDB\n",
      "  Database File: c:\\Users\\mvzie\\Documents\\AI Agent Experiment\\animal_shelter.duckdb\n",
      "\n",
      "Agent Parameters:\n",
      "  Temperature: 0.3\n",
      "  Max Tokens: 1000\n",
      "  Timeout: 30 seconds\n",
      "\n",
      "Training Data:\n",
      "  Few-shot Examples: 5\n",
      "  System Prompt Size: 2,576 characters\n",
      "  Total Prompt Size: 6,707 characters\n",
      "\n",
      "Created: 2026-01-02T19:08:32.752722\n",
      "Version: 1.0\n",
      "\n",
      "Configuration saved to: mindsdb_agent_config.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print final agent configuration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL AGENT CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAgent Details:\")\n",
    "print(f\"  Name: {agent_definition['name']}\")\n",
    "print(f\"  Type: {agent_definition['type']}\")\n",
    "print(f\"  Description: {agent_definition['description']}\")\n",
    "print(f\"  Database: {agent_definition['database_type'].upper()}\")\n",
    "print(f\"  Database File: {agent_definition['database']}\")\n",
    "print(f\"\\nAgent Parameters:\")\n",
    "print(f\"  Temperature: {agent_definition['temperature']}\")\n",
    "print(f\"  Max Tokens: {agent_definition['max_tokens']}\")\n",
    "print(f\"  Timeout: {agent_definition['timeout']} seconds\")\n",
    "print(f\"\\nTraining Data:\")\n",
    "print(f\"  Few-shot Examples: {len(training_examples)}\")\n",
    "print(f\"  System Prompt Size: {len(system_prompt):,} characters\")\n",
    "print(f\"  Total Prompt Size: {len(full_system_prompt):,} characters\")\n",
    "print(f\"\\nCreated: {agent_definition['created_at']}\")\n",
    "print(f\"Version: {agent_definition['version']}\")\n",
    "print(f\"\\nConfiguration saved to: mindsdb_agent_config.json\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201e700",
   "metadata": {},
   "source": [
    "## Section 11: Next Steps - Ready for Validation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6aac5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NEXT STEPS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Agent Creation Complete!\n",
      "\n",
      "You can now proceed with Step 8.3 - Agent Validation Testing:\n",
      "\n",
      "1. Open: test_validate_mindsdb_agent_v2.ipynb\n",
      "2. The validation notebook will:\n",
      "   - Load this agent configuration\n",
      "   - Run 20 iterations per test case (220 total)\n",
      "   - Compare agent-generated SQL to ground truth\n",
      "   - Calculate accuracy metrics\n",
      "   - Generate performance report\n",
      "\n",
      "3. Success Criteria:\n",
      "   - Overall accuracy: >80%\n",
      "   - All test cases showing improvement\n",
      "   - No critical errors in SQL generation\n",
      "\n",
      "4. Key Metrics Tracked:\n",
      "   - Exact match: Generated SQL = ground truth SQL\n",
      "   - Semantic equivalence: Same results, different SQL\n",
      "   - Result accuracy: Correct output regardless of SQL\n",
      "\n",
      "Agent is now ready for validation! ðŸš€\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "âœ“ Agent Creation Complete!\n",
    "\n",
    "You can now proceed with Step 8.3 - Agent Validation Testing:\n",
    "\n",
    "1. Open: test_validate_mindsdb_agent_v2.ipynb\n",
    "2. The validation notebook will:\n",
    "   - Load this agent configuration\n",
    "   - Run 20 iterations per test case (220 total)\n",
    "   - Compare agent-generated SQL to ground truth\n",
    "   - Calculate accuracy metrics\n",
    "   - Generate performance report\n",
    "\n",
    "3. Success Criteria:\n",
    "   - Overall accuracy: >80%\n",
    "   - All test cases showing improvement\n",
    "   - No critical errors in SQL generation\n",
    "\n",
    "4. Key Metrics Tracked:\n",
    "   - Exact match: Generated SQL = ground truth SQL\n",
    "   - Semantic equivalence: Same results, different SQL\n",
    "   - Result accuracy: Correct output regardless of SQL\n",
    "\n",
    "Agent is now ready for validation! ðŸš€\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742e1c1",
   "metadata": {},
   "source": [
    "## Section 12 (Optional): Integrate Ollama/Mistral for SQL Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e52dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OLLAMA/MISTRAL INTEGRATION SETUP\n",
      "================================================================================\n",
      "\n",
      "Ollama server is running on http://127.0.0.1:11434\n",
      "Available models: ['mistral:latest']\n",
      "\n",
      "âœ“ Mistral model is available\n",
      "\n",
      "âœ“ Ollama is ready for SQL generation!\n"
     ]
    }
   ],
   "source": [
    "# Setup Ollama/Mistral integration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OLLAMA/MISTRAL INTEGRATION SETUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import requests\n",
    "\n",
    "# Test Ollama connection\n",
    "ollama_url = \"http://127.0.0.1:11434\"\n",
    "ollama_model = \"mistral:latest\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{ollama_url}/api/tags\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        available_models = [m['name'] for m in models.get('models', [])]\n",
    "        \n",
    "        print(f\"\\nOllama server is running on {ollama_url}\")\n",
    "        print(f\"Available models: {available_models}\")\n",
    "        \n",
    "        if ollama_model in available_models:\n",
    "            print(f\"\\nâœ“ Mistral model is available\")\n",
    "            ollama_available = True\n",
    "        else:\n",
    "            print(f\"\\nâœ— Mistral model not found\")\n",
    "            print(f\"Available: {available_models}\")\n",
    "            ollama_available = False\n",
    "    else:\n",
    "        print(f\"Error connecting to Ollama: {response.status_code}\")\n",
    "        ollama_available = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Ollama server is not running: {str(e)[:50]}\")\n",
    "    print(f\"Start Ollama with: ollama serve\")\n",
    "    print(f\"Pull Mistral with: ollama pull mistral\")\n",
    "    ollama_available = False\n",
    "\n",
    "if ollama_available:\n",
    "    print(\"\\nâœ“ Ollama is ready for SQL generation!\")\n",
    "else:\n",
    "    print(\"\\nOllama is not available - configure OpenAI API as alternative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfbe9dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEXT-TO-SQL WITH MISTRAL\n",
      "================================================================================\n",
      "\n",
      "Testing Mistral SQL generation...\n",
      "Question: What are the different animal outcomes and how many animals have each outcome?...\n",
      "Generating SQL (may take 30-60 seconds)...\n",
      "\n",
      "Generated SQL: SELECT outcome_type, COUNT(*) as total,\n",
      "       ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 1) as percentage,\n",
      "       ROUND(AVG(days_in_shelter), 1)...\n",
      "Success! Query returned 12 rows\n"
     ]
    }
   ],
   "source": [
    "# Create text-to-SQL function using Mistral\n",
    "if ollama_available:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEXT-TO-SQL WITH MISTRAL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    def mistral_text_to_sql(question, schema_context, system_prompt):\n",
    "        \"\"\"Generate SQL from natural language using Mistral\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Generate SQL query:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            payload = {\n",
    "                \"model\": ollama_model,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"temperature\": 0.3\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{ollama_url}/api/generate\",\n",
    "                json=payload,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                generated_text = result['response'].strip()\n",
    "                \n",
    "                # Extract SQL from response (may be wrapped in markdown)\n",
    "                import re\n",
    "                \n",
    "                # Try markdown code block first\n",
    "                sql_match = re.search(r'```(?:sql)?\\s*(SELECT.*?);?\\s*```', generated_text, re.DOTALL | re.IGNORECASE)\n",
    "                \n",
    "                if not sql_match:\n",
    "                    # Try standalone SQL\n",
    "                    sql_match = re.search(r'(SELECT\\s+.*?;)', generated_text, re.DOTALL | re.IGNORECASE)\n",
    "                \n",
    "                if sql_match:\n",
    "                    sql = sql_match.group(1)\n",
    "                    sql = sql.replace('```', '').strip()\n",
    "                    if not sql.endswith(';'):\n",
    "                        sql += ';'\n",
    "                    return sql\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Test with a sample question\n",
    "    print(\"\\nTesting Mistral SQL generation...\")\n",
    "    test_question = training_examples[0]['question']\n",
    "    \n",
    "    print(f\"Question: {test_question[:80]}...\")\n",
    "    print(f\"Generating SQL (may take 30-60 seconds)...\")\n",
    "    \n",
    "    generated_sql = mistral_text_to_sql(test_question, schema_context, full_system_prompt)\n",
    "    \n",
    "    if generated_sql:\n",
    "        print(f\"\\nGenerated SQL: {generated_sql[:150]}...\")\n",
    "        \n",
    "        try:\n",
    "            result_df = conn.execute(generated_sql).df()\n",
    "            print(f\"Success! Query returned {len(result_df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"SQL execution error: {str(e)[:80]}\")\n",
    "    else:\n",
    "        print(\"Failed to extract SQL from response\")\n",
    "else:\n",
    "    print(\"Ollama/Mistral not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d052c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MISTRAL SQL GENERATION VALIDATION\n",
      "Testing all 11 ground truth test cases...\n",
      "================================================================================\n",
      "\n",
      "Q 1: Outcome Distribution...\n",
      "  PASS: Generated 12/12 rows\n",
      "\n",
      "Q 2: Top Breed Groups Overall...\n",
      "  PASS: Generated 7/7 rows\n",
      "\n",
      "Q 3: Adoption Success Rate by Primary Breed...\n",
      "  PASS: Generated 5/5 rows\n",
      "\n",
      "Q 4: High Demand Animals (Short Stay Before Adoption/Transfer) by...\n",
      "  PASS: Generated 5/5 rows\n",
      "\n",
      "Q 5: High Need Animals (Longest Stay and Problem Conditions)...\n",
      "  ERROR: Parser Error: syntax error at or near \")\"\n",
      "\n",
      "Q 6: Sick and Injured Animals Outcomes...\n",
      "  PASS: Generated 39/39 rows\n",
      "\n",
      "Q 7: Stay Duration by Outcome...\n",
      "  FAIL: Generated 12/43 rows\n",
      "\n",
      "Q 8: Monthly Outcome Trends 2016...\n",
      "  ERROR: Binder Error: Table \"f\" does not have a column named \"date_k\n",
      "\n",
      "Q 9: Gender Distribution by Outcome...\n",
      "  FAIL: Generated 24/34 rows\n",
      "\n",
      "Q10: Intake Type Analysis...\n",
      "  ERROR: Could not extract SQL\n",
      "\n",
      "Q11: Reproductive Status by Age Group...\n",
      "  PASS: Generated 4/4 rows\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE VALIDATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Test Results:\n",
      "  Passed: 6/11\n",
      "  Failed: 2/11\n",
      "  Errors: 3/11\n",
      "\n",
      "Accuracy: 54.5%\n",
      "\n",
      "Failed/Error Tests:\n",
      "  Q 5: High Need Animals (Longest Stay and Problem Condit - ERROR\n",
      "  Q 7: Stay Duration by Outcome - FAIL\n",
      "  Q 8: Monthly Outcome Trends 2016 - ERROR\n",
      "  Q 9: Gender Distribution by Outcome - FAIL\n",
      "  Q10: Intake Type Analysis - ERROR\n",
      "\n",
      "âš  IMPROVEMENT NEEDED - 55% accuracy (6/11)\n"
     ]
    }
   ],
   "source": [
    "# Full validation of Mistral SQL generation on ALL 11 test cases\n",
    "if ollama_available:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE MISTRAL SQL GENERATION VALIDATION\")\n",
    "    print(\"Testing all 11 ground truth test cases...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    mistral_results = {}\n",
    "    all_test_cases = test_cases_data['test_cases']  # All 11 test cases\n",
    "    \n",
    "    for tc in all_test_cases:\n",
    "        test_id = tc['id']\n",
    "        question = tc['natural_language_question']\n",
    "        expected_sql = tc['ground_truth_sql']\n",
    "        expected_rows = tc['result_count']\n",
    "        \n",
    "        print(f\"\\nQ{test_id:2d}: {tc['name'][:60]}...\")\n",
    "        \n",
    "        try:\n",
    "            generated_sql = mistral_text_to_sql(question, schema_context, full_system_prompt)\n",
    "            \n",
    "            if generated_sql:\n",
    "                result_df = conn.execute(generated_sql).df()\n",
    "                actual_rows = len(result_df)\n",
    "                \n",
    "                # Check if result count matches\n",
    "                matches = actual_rows == expected_rows\n",
    "                status = \"PASS\" if matches else \"FAIL\"\n",
    "                \n",
    "                print(f\"  {status}: Generated {actual_rows}/{expected_rows} rows\")\n",
    "                \n",
    "                mistral_results[test_id] = {\n",
    "                    'status': 'pass' if matches else 'fail',\n",
    "                    'name': tc['name'],\n",
    "                    'generated_sql': generated_sql[:80],\n",
    "                    'expected_sql': expected_sql[:80],\n",
    "                    'actual_rows': actual_rows,\n",
    "                    'expected_rows': expected_rows\n",
    "                }\n",
    "            else:\n",
    "                print(f\"  ERROR: Could not extract SQL\")\n",
    "                mistral_results[test_id] = {'status': 'error', 'name': tc['name']}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {str(e)[:60]}\")\n",
    "            mistral_results[test_id] = {'status': 'error', 'name': tc['name'], 'error': str(e)[:50]}\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE VALIDATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    passed = sum(1 for r in mistral_results.values() if r['status'] == 'pass')\n",
    "    failed = sum(1 for r in mistral_results.values() if r['status'] == 'fail')\n",
    "    errors = sum(1 for r in mistral_results.values() if r['status'] == 'error')\n",
    "    total = len(mistral_results)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Passed: {passed}/{total}\")\n",
    "    print(f\"  Failed: {failed}/{total}\")\n",
    "    print(f\"  Errors: {errors}/{total}\")\n",
    "    print(f\"\\nAccuracy: {100*passed/total:.1f}%\")\n",
    "    \n",
    "    if failed > 0 or errors > 0:\n",
    "        print(f\"\\nFailed/Error Tests:\")\n",
    "        for test_id, result in mistral_results.items():\n",
    "            if result['status'] != 'pass':\n",
    "                print(f\"  Q{test_id:2d}: {result['name'][:50]} - {result['status'].upper()}\")\n",
    "    \n",
    "    if passed == total:\n",
    "        print(\"\\nâœ“ PERFECT! Mistral is generating correct SQL for all 11 test cases!\")\n",
    "    elif passed >= 9:\n",
    "        print(f\"\\nâœ“ EXCELLENT! Mistral is {100*passed/total:.0f}% accurate ({passed}/{total})\")\n",
    "    elif passed >= 7:\n",
    "        print(f\"\\nâš  GOOD! Mistral is {100*passed/total:.0f}% accurate ({passed}/{total})\")\n",
    "    elif passed > 0:\n",
    "        print(f\"\\nâš  IMPROVEMENT NEEDED - {100*passed/total:.0f}% accuracy ({passed}/{total})\")\n",
    "    else:\n",
    "        print(\"\\nâœ— Mistral SQL generation needs significant improvement\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ab875",
   "metadata": {},
   "source": [
    "## Section 12: Close Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a20c7bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ DuckDB connection closed\n",
      "âœ“ Agent creation notebook complete\n"
     ]
    }
   ],
   "source": [
    "# Close DuckDB connection\n",
    "conn.close()\n",
    "print(\"\\nâœ“ DuckDB connection closed\")\n",
    "print(\"âœ“ Agent creation notebook complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
